{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshayaa1010/GenAI-Customer-Support-Quality-Auditor/blob/main/transcribe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwaSGYYfW6zm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"1755884171.51632.mp3\" --model medium.en"
      ],
      "metadata": {
        "id": "Xix9BPZ6YEwJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_subtitle(text):\n",
        "    # remove timestamps\n",
        "    text = re.sub(r\"\\d+:\\d+:\\d+.\\d+ --> .*\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\\n\", \"\", text)\n",
        "\n",
        "    # remove html tags if any\n",
        "    text = re.sub(r\"<.*?>\",\"\", text)\n",
        "\n",
        "    # remove filler words\n",
        "    text = re.sub(r\"\\b(uh|um|ah|er|hmm|mmm|you know|like|really|probably)\\b\",\"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # remove extra spaces\n",
        "    text = re.sub(r\"\\s+\",\" \", text)\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "2REBdunpo5Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"1755884171.51632.vtt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "\n",
        "clean_transcript = clean_subtitle(raw)\n",
        "print(clean_transcript)\n"
      ],
      "metadata": {
        "id": "Ipn_X0irrCvE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_turns(text):\n",
        "    turns = re.split(r'(Yeah|Okay|So|Listen|Well|Thanks|Bye)', text)\n",
        "    return [t.strip() for t in turns if len(t.strip()) > 3]\n",
        "\n",
        "turns = split_turns(clean_transcript)\n",
        "len(turns), turns[:10]"
      ],
      "metadata": {
        "id": "Hfo4vFpAr8kV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = []\n",
        "\n",
        "speaker = \"Agent\"  # start assuming Agent\n",
        "for i, t in enumerate(turns):\n",
        "    conversation.append({\"speaker\": speaker, \"text\": t})\n",
        "\n",
        "    # toggle speaker\n",
        "    speaker = \"Customer\" if speaker==\"Agent\" else \"Agent\"\n",
        "\n",
        "conversation[:6]\n"
      ],
      "metadata": {
        "id": "90so-9fwuCtw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"structured_conversation.json\",\"w\") as f:\n",
        "    json.dump(conversation,f,indent=4)\n"
      ],
      "metadata": {
        "id": "M1hSz1FYuJrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load file\n",
        "with open(\"structured_conversation.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "cleaned_data = []\n",
        "\n",
        "for entry in data:\n",
        "    text = entry[\"text\"]\n",
        "\n",
        "    text = re.sub(r\"WEBVTT\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}.\\d+\\s-->\\s\\d{2}:\\d{2}.\\d+\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}.\\d+\", \"\", text)\n",
        "    text = re.sub(r\",\\s*,\", \",\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    if text:\n",
        "        cleaned_data.append({\n",
        "            \"speaker\": entry[\"speaker\"],\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(cleaned_data)\n",
        "\n",
        "df.to_csv(\"cleaned_calls.csv\", index=False)\n",
        "\n",
        "print(\"Cleaning Done! Saved as cleaned_calls.csv\")\n"
      ],
      "metadata": {
        "id": "7D89xjW52nS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open(\"1755884171.51632.vtt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = re.sub(r\"WEBVTT\",\"\", text)\n",
        "text = re.sub(r\"\\d{2}:\\d{2}.\\d+\\s-->\\s\\d{2}:\\d{2}.\\d+\",\"\", text)\n",
        "text = re.sub(r\"\\d{2}:\\d{2}.\\d+\",\"\", text)\n",
        "text = re.sub(r\"\\s+\",\" \", text).strip()\n",
        "\n",
        "with open(\"cleaned_call.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(\"Clean file saved as cleaned_call.txt\")"
      ],
      "metadata": {
        "id": "5xUbPk3g31OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv(\"cleaned_calls.csv\")\n",
        "\n",
        "print(\"Before Cleaning:\", df.shape)\n",
        "\n",
        "def clean_transcript(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove --> 00:13. / --> 09:10. etc\n",
        "    text = re.sub(r'-->\\s*\\d{2}:\\d{2}\\.?\\d*', '', text)\n",
        "\n",
        "    # Remove standalone timestamps like 00:13 , 01:04 etc\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\.?\\d*', '', text)\n",
        "\n",
        "    # Remove leftover arrows\n",
        "    text = re.sub(r'-->', '', text)\n",
        "\n",
        "    # Remove leading punctuation like \", \" or \". \"\n",
        "    text = re.sub(r'^[,\\.]\\s*', '', text)\n",
        "\n",
        "    # Remove repetitive punctuation ,, ...\n",
        "    text = re.sub(r'[,\\.]{2,}', '.', text)\n",
        "\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_transcript)\n",
        "\n",
        "# Drop empty rows\n",
        "df = df[df['clean_text'] != \"\"]\n",
        "\n",
        "print(\"After Cleaning:\", df.shape)\n",
        "\n",
        "# Save cleaned dataset\n",
        "df[['speaker','clean_text']].to_csv(\"cleaned_calls.csv\", index=False)\n",
        "\n",
        "print(\"Cleaning completed! Saved as cleaned_calls.csv\")\n"
      ],
      "metadata": {
        "id": "S3JWj1bh4-_o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate torch\n"
      ],
      "metadata": {
        "id": "X12E_asXnAwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"cleaned_calls.csv\")\n",
        "\n",
        "conversation_text = \"\"\n",
        "for _, row in df.iterrows():\n",
        "    conversation_text += f\"{row['speaker']}: {row['clean_text']}\\n\"\n",
        "\n",
        "print(conversation_text[:1000])  # preview only\n"
      ],
      "metadata": {
        "id": "c8aVLlY3nfUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "3sEipAj6nlZd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"cleaned_calls.csv\")\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "WxYSUuznv3mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_text = \"\"\n",
        "for _, row in df.iterrows():\n",
        "    conversation_text += f\"{row['speaker']}: {row['clean_text']}\\n\"\n",
        "\n",
        "print(conversation_text[:500])\n"
      ],
      "metadata": {
        "id": "mjTkXjTav9l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=1200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        chunks.append(text[start:start + chunk_size])\n",
        "        start += chunk_size\n",
        "    return chunks\n",
        "\n",
        "conversation_chunks = chunk_text(conversation_text)\n",
        "print(len(conversation_chunks))\n"
      ],
      "metadata": {
        "id": "k2nM9czAwGMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_chunk(conversation_chunk):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "<|system|>\n",
        "You are a customer support quality auditor.\n",
        "\n",
        "<|user|>\n",
        "Evaluate the AGENT in the following conversation.\n",
        "\n",
        "1. Give scores from 1 to 100 for:\n",
        "   - Empathy\n",
        "   - Professionalism\n",
        "\n",
        "2. For Compliance, classify the agent behavior as one of:\n",
        "   - PASS (fully compliant)\n",
        "   - WARN (minor issues or risky statements)\n",
        "   - FAIL (clear policy or ethical violations)\n",
        "\n",
        "Respond STRICTLY in this format:\n",
        "Empathy Score: <number>\n",
        "Professionalism Score: <number>\n",
        "Compliance Status: <PASS / WARN / FAIL>\n",
        "\n",
        "Conversation:\n",
        "{conversation_chunk}\n",
        "\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.2,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "3uukEKzTxpbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_output = score_chunk(conversation_chunks[0])\n",
        "print(test_output)\n"
      ],
      "metadata": {
        "id": "vPA275Adxxdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# --- Start of code to regenerate 'cleaned_calls.csv' ---\n",
        "# This section combines logic from previous cells (M1hSz1FYuJrK, 7D89xjW52nS9, S3JWj1bh4-_o)\n",
        "# to ensure 'cleaned_calls.csv' is available.\n",
        "\n",
        "# NOTE: This assumes 'structured_conversation.json' exists. If not, further prior cells would need to be run.\n",
        "# Load 'structured_conversation.json'\n",
        "with open(\"structured_conversation.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Process data as in cell 7D89xjW52nS9\n",
        "cleaned_data_step1 = []\n",
        "for entry in data:\n",
        "    text = entry[\"text\"]\n",
        "\n",
        "    text = re.sub(r\"WEBVTT\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}.\\d+\\s-->\\s\\d{2}:\\d{2}.\\d+\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}.\\d+\", \"\", text)\n",
        "    text = re.sub(r\",\\s*,\", \",\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    if text:\n",
        "        cleaned_data_step1.append({\n",
        "            \"speaker\": entry[\"speaker\"],\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "df_intermediate = pd.DataFrame(cleaned_data_step1)\n",
        "\n",
        "# Define clean_transcript function as in cell S3JWj1bh4-_o\n",
        "def clean_transcript(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove --> 00:13. / --> 09:10. etc\n",
        "    text = re.sub(r'-->\\s*\\d{2}:\\d{2}\\.?\\d*', '', text)\n",
        "\n",
        "    # Remove standalone timestamps like 00:13 , 01:04 etc\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\.?\\d*', '', text)\n",
        "\n",
        "    # Remove leftover arrows\n",
        "    text = re.sub(r'-->', '', text)\n",
        "\n",
        "    # Remove leading punctuation like \", \" or \". \"\n",
        "    text = re.sub(r'^[\\.,]\\s*', '', text)\n",
        "\n",
        "    # Remove repetitive punctuation ,, ...\n",
        "    text = re.sub(r'[\\.,]{2,}', '.', text)\n",
        "\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply further cleaning and save as 'cleaned_calls.csv' as in cell S3JWj1bh4-_o\n",
        "df_final = df_intermediate.copy()\n",
        "df_final['clean_text'] = df_final['text'].apply(clean_transcript)\n",
        "\n",
        "# Drop empty rows\n",
        "df_final = df_final[df_final['clean_text'] != \"\"]\n",
        "\n",
        "# Save cleaned dataset, overwriting previous if it existed\n",
        "df_final[['speaker','clean_text']].to_csv(\"cleaned_calls.csv\", index=False)\n",
        "\n",
        "# --- End of code to regenerate 'cleaned_calls.csv' ---\n",
        "\n",
        "# Now, the original part of this cell can run, as 'cleaned_calls.csv' should exist\n",
        "df = pd.read_csv(\"cleaned_calls.csv\")\n",
        "\n",
        "conversation_text = \"\"\n",
        "for _, row in df.iterrows():\n",
        "    conversation_text += f\"{row['speaker']}: {row['clean_text']}\\n\"\n",
        "\n",
        "def chunk_text(text, chunk_size=1200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        chunks.append(text[start:start + chunk_size])\n",
        "        start += chunk_size\n",
        "    return chunks\n",
        "\n",
        "conversation_chunks = chunk_text(conversation_text)\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "for i, chunk in enumerate(conversation_chunks):\n",
        "    print(f\"Scoring chunk {i+1}/{len(conversation_chunks)}\")\n",
        "    output = score_chunk(chunk)\n",
        "    all_outputs.append(output)"
      ],
      "metadata": {
        "id": "CgDMeXs5ZA4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_scores(text):\n",
        "    empathy = re.search(r\"Empathy Score:\\s*(\\d+)\", text)\n",
        "    professionalism = re.search(r\"Professionalism Score:\\s*(\\d+)\", text)\n",
        "    compliance = re.search(r\"Compliance Status:\\s*(PASS|WARN|FAIL)\", text)\n",
        "\n",
        "    return {\n",
        "        \"empathy\": int(empathy.group(1)) if empathy else None,\n",
        "        \"professionalism\": int(professionalism.group(1)) if professionalism else None,\n",
        "        \"compliance\": compliance.group(1) if compliance else None\n",
        "    }"
      ],
      "metadata": {
        "id": "voJQmJhScKuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [parse_scores(o) for o in all_outputs]\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "id": "psjTufqGcNxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "empathy_scores = [s['empathy'] for s in scores if s['empathy'] is not None]\n",
        "professionalism_scores = [s['professionalism'] for s in scores if s['professionalism'] is not None]\n",
        "\n",
        "avg_empathy = round(np.mean(empathy_scores), 2)\n",
        "avg_professionalism = round(np.mean(professionalism_scores), 2)"
      ],
      "metadata": {
        "id": "wde4Fd2ZcTsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compliance_values = [s['compliance'] for s in scores]\n",
        "\n",
        "if \"FAIL\" in compliance_values:\n",
        "    final_compliance = \"FAIL\"\n",
        "elif \"WARN\" in compliance_values:\n",
        "    final_compliance = \"WARN\"\n",
        "else:\n",
        "    final_compliance = \"PASS\""
      ],
      "metadata": {
        "id": "82XnfBEZcXYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FINAL AUDIT RESULT\")\n",
        "print(\"-------------------\")\n",
        "print(f\"Average Empathy Score: {avg_empathy}\")\n",
        "print(f\"Average Professionalism Score: {avg_professionalism}\")\n",
        "print(f\"Overall Compliance Status: {final_compliance}\")"
      ],
      "metadata": {
        "id": "KQdAxb8Dcbka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y2Q3NH6jtGWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community langchain-core faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yaSkxyeRvfpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community langchain-core langchain-text-splitters faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "apjf52n9whsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "  torch torchvision torchaudio \\\n",
        "  transformers \\\n",
        "  sentence-transformers \\\n",
        "  langchain==1.2.9 \\\n",
        "  langchain-community==0.4.1 \\\n",
        "  langchain-core==1.2.9 \\\n",
        "  langchain-text-splitters==1.1.0 \\\n",
        "  langchain-huggingface==1.2.0 \\\n",
        "  faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H395Z5nixf9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d438f8d4"
      },
      "source": [
        "### Creating `policies.txt`\n",
        "\n",
        "To resolve the `FileNotFoundError`, I'm creating a `policies.txt` file with some example policy content. In a real scenario, this file would contain your actual company policies relevant to customer service interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57947376"
      },
      "source": [
        "policies_content = \"\"\"\n",
        "**Company Policy 1: Customer Empathy**\n",
        "Agents must demonstrate active listening and empathy towards customers. Acknowledge customer feelings and show understanding of their situation. Use phrases like \"I understand this is frustrating\" or \"I apologize for the inconvenience you've experienced.\" Strive to connect with the customer on an emotional level while maintaining professionalism.\n",
        "\n",
        "**Company Policy 2: Professionalism in Communication**\n",
        "All agent interactions must be professional, courteous, and respectful. Avoid slang, jargon, or overly casual language. Maintain a calm and composed demeanor, even when customers are distressed. Ensure clear and concise communication.\n",
        "\n",
        "**Company Policy 3: Data Privacy and Confidentiality**\n",
        "Agents must never share sensitive customer information with unauthorized parties. Always verify customer identity before discussing account-specific details. Adhere to all data protection regulations (e.g., GDPR, CCPA). Do not ask for passwords or other highly sensitive credentials.\n",
        "\n",
        "**Company Policy 4: Accurate Information Provision**\n",
        "Agents must provide accurate and up-to-date information regarding products, services, and company procedures. If unsure, consult reliable internal resources or escalate the query to a supervisor. Misinformation can lead to compliance violations and customer dissatisfaction.\n",
        "\n",
        "**Company Policy 5: Resolution and Follow-up**\n",
        "Agents should aim for first-contact resolution whenever possible. Clearly communicate next steps and timelines if an issue cannot be resolved immediately. Follow up with customers as promised to ensure satisfaction and provide updates.\n",
        "\n",
        "**Company Policy 6: Ethical Conduct**\n",
        "Agents must act with integrity and honesty in all interactions. Do not make false promises or misleading statements. Report any suspicious activity or potential breaches of conduct to management.\n",
        "\n",
        "**Company Policy 7: Prohibited Language**\n",
        "Agents are strictly prohibited from using offensive, discriminatory, or aggressive language. Avoid political or personal opinions during customer interactions.\n",
        "\n",
        "**Company Policy 8: Compliance with Legal Requirements**\n",
        "All conversations and actions must comply with relevant legal and regulatory frameworks, including consumer protection laws and industry-specific regulations.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"policies.txt\", \"w\") as f:\n",
        "    f.write(policies_content)\n",
        "\n",
        "print(\"policies.txt created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Load policy text\n",
        "with open(\"policies.txt\", \"r\") as f:\n",
        "    policy_text = f.read()\n",
        "\n",
        "# Split policy text\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "docs = text_splitter.create_documents([policy_text])\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "print(\"âœ… RAG policy vector store ready\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3nEG0JKZtLmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_policy(conversation_chunk):\n",
        "    retrieved_docs = vectorstore.similarity_search(conversation_chunk, k=2)\n",
        "    return \"\\n\".join([doc.page_content for doc in retrieved_docs])\n"
      ],
      "metadata": {
        "id": "qgAAfNeqy06J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json # Ensure json is imported\n",
        "import re # Import re for regex parsing\n",
        "\n",
        "def contextual_audit(conversation_chunk):\n",
        "    policy_context = retrieve_policy(conversation_chunk)\n",
        "\n",
        "    # Simplified and more direct prompt, removing system/user/assistant tags\n",
        "    prompt = f\"\"\"\n",
        "As a customer support quality auditor, your task is to analyze the agent's performance in the conversation below, considering the provided company policies.\n",
        "\n",
        "Company Policies:\n",
        "{policy_context}\n",
        "\n",
        "Conversation:\n",
        "{conversation_chunk}\n",
        "\n",
        "Evaluate the AGENT and provide scores for Empathy (1-100) and Professionalism (1-100), along with a Compliance status (PASS, WARN, or FAIL).\n",
        "\n",
        "Your response MUST be a JSON object ONLY, with no other text, explanations, or formatting. Generate only the JSON. Do not include any other text.\n",
        "\n",
        "{{\n",
        "  \"Empathy\": <score 1-100>,\n",
        "  \"Professionalism\": <score 1-100>,\n",
        "  \"Compliance\": \"<PASS/WARN/FAIL>\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.2,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the part generated by the model, excluding the input prompt\n",
        "    prompt_text_len = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
        "    generated_text = full_response[prompt_text_len:].strip()\n",
        "\n",
        "    # Attempt to parse the generated_text directly as JSON first\n",
        "    try:\n",
        "        parsed_json = json.loads(generated_text)\n",
        "        return json.dumps(parsed_json, indent=2) # Return pretty-printed JSON\n",
        "    except json.JSONDecodeError:\n",
        "        # If direct parsing fails, try to find a JSON block using regex (e.g., between curly braces)\n",
        "        json_match = re.search(r'\\{.*\\}', generated_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_string = json_match.group(0)\n",
        "            try:\n",
        "                parsed_json = json.loads(json_string)\n",
        "                return json.dumps(parsed_json, indent=2)\n",
        "            except json.JSONDecodeError:\n",
        "                return f\"JSON parsing failed after attempting regex. Full generated text: {generated_text}\"\n",
        "        else:\n",
        "            # If no JSON block is found, return the full generated text for inspection\n",
        "            return f\"No JSON block found. Full generated text: {generated_text}\"\n"
      ],
      "metadata": {
        "id": "WqauBsqsy6dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generated-code-3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"cleaned_calls.csv\")\n",
        "\n",
        "conversation_text = \"\"\n",
        "for _, row in df.iterrows():\n",
        "    conversation_text += f\"{row['speaker']}: {row['clean_text']}\\n\"\n",
        "\n",
        "def chunk_text(text, chunk_size=1200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        chunks.append(text[start:start + chunk_size])\n",
        "        start += chunk_size\n",
        "    return chunks\n",
        "\n",
        "conversation_chunks = chunk_text(conversation_text)\n",
        "print(\"conversation_chunks regenerated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c823ebc"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_contextual_audit_output"
      },
      "source": [
        "test_contextual_audit_output = contextual_audit(conversation_chunks[0])\n",
        "print(test_contextual_audit_output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}